<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GitHub-Hexo博客网站搭建（二）</title>
    <link href="/2022/11/11/GitHub-Hexo%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2022/11/11/GitHub-Hexo%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><p>Hexo网页无法正常加载图片</p><p>解决方法：</p><ol><li><p>_config.yml和主题的_config.theme.yml里的<code>post_asset_folder</code>设置为true（这样每篇文章都会有对应同名的文件夹，图片就放在这里面）</p></li><li><p><code>npm install https://github.com/CodeFalling/hexo-asset-image --save</code>（一定要有前面的网址，不能只有hexo-asset-image，这是个坑，害我搞了好久）</p></li><li><p>markdown正常插入图片（即<code>![image_introduction](blog_name/image_name.jpg)</code>）</p></li></ol><h1 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h1><p>Hexo网页无法正常加载公式</p><p>解决方法</p><ol><li><p>在theme文件夹里找到主题的配置文件_config.yml</p></li><li><p>把<code>mathjax </code>下面的<code>enable</code> 设置为 true（到这里只解决了行间公式，即<code>$fomula$</code>）</p></li><li><p>在有数学公式的markdown文件里的front-matter（即最开头那块）里加上<code>mathjax: true</code>（到这里才更进一步解决了公式块，即<code>$$fomula$$</code>）</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>网站搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer笔记</title>
    <link href="/2022/11/11/Transformer%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/11/11/Transformer%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<img src="/2022/11/11/Transformer%E7%AC%94%E8%AE%B0/transformer_model.jpg" title alt="transformer_model" data-align="center"><h1 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h1><h2 id="One-Hot-Encoding"><a href="#One-Hot-Encoding" class="headerlink" title="One-Hot Encoding"></a>One-Hot Encoding</h2><p>将输入单词用one-hot形式编码成序列向量，向量长度就是预定义的词汇表中拥有的词汇量，向量在这一维中的值只有一个位置是1，其余都是0，1对应的位置就是词汇表中表示这个单词的地方</p><p>劣势：稀疏；长</p><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>设计一个可学习的权重矩阵W，将one-hot词向量与这个矩阵进行点乘，得到新的表示结果</p><p>优势：降维</p><h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><p>预定义函数，通过函数计算出位置信息<br>$PE_{(pos,2i)}=sin(pos/10000^{2i/d})$</p><p>$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d})$<br>这种编码方式保证了不同位置在所有维度上不会被编码到完全一样的值</p><h2 id="Transformer对输入的操作概括"><a href="#Transformer对输入的操作概括" class="headerlink" title="Transformer对输入的操作概括"></a>Transformer对输入的操作概括</h2><p>word embedding + position embedding = final representation</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/e6b5b463cf7b">Transformer 修炼之道（一）、Input Embedding</a></p><h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p>Encoder由多个（通常6个）层组成，每个层内部都有<strong>残差连接</strong>和<strong>归一化操作</strong>，主要内容包括<strong>多头自注意力层</strong>（Multi-Head Self-Attention）和<strong>前向反馈网络层</strong>（Feed Forward Network），前者用于捕捉特征之间的关系，后者是进一步编码学习，实质上等同于MLP（Multi-Layer Perception）</p><h2 id="Multi-Head-Self-Attention"><a href="#Multi-Head-Self-Attention" class="headerlink" title="Multi-Head Self-Attention"></a>Multi-Head Self-Attention</h2><p>多头（多个，通常为8）</p><ol><li><p>对输入做不同的线性变换生成<strong>Q（Query）、K（Key）、V（Value）</strong></p></li><li><p><strong>Scaled Dot-Product Attention</strong>（对Q、K、V进行计算得到输出）</p><p>$$<br>Attention(Q,K,V)=softmax(\frac{Q K^T}{\sqrt{d_k}})V \<br>d_k是Q,K矩阵的列数，即向量维度<br>$$</p><p>除以\sqrt{d_k}的原因是防止Q和K的点积值过大（\sqrt{d_k}即Q的列数，也是K的行数），避免在经过softmax后梯度太小</p></li><li><p>将多个自注意力的输出拼接起来，再进行线性变换</p></li></ol><h2 id="Feed-Forward-Network（FFN）"><a href="#Feed-Forward-Network（FFN）" class="headerlink" title="Feed Forward Network（FFN）"></a>Feed Forward Network（FFN）</h2><p>两个全连接层，其中一个带ReLU激活，两层中间有Dropout<br>$FFN(x)=max(0,xW_1+b_1)W_2+b_2$</p><h2 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h2><p>$$<br>LayerNorm(X+MultiHeadAttention(X)) \<br>LayerNorm(X+FeedForward(X))<br>$$</p><p>Add：输入加输出（残差连接）</p><p>Norm：Layer Normalization，通常为$\frac{x-\mu}{\sigma}$，$\mu$表示均值$\sigma$表示标准差</p><h2 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/81901d3d3f8e">Transformer 修炼之道（二）、Encoder</a></p><h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><h2 id="Multi-Head-Attention-with-Mask"><a href="#Multi-Head-Attention-with-Mask" class="headerlink" title="Multi-Head Attention with Mask"></a>Multi-Head Attention with Mask</h2><p>mask：忽略某些位置（要预测的之后的内容），不计算与其相关的注意力权重（下三角矩阵，以极大负值代替0，从而使得其经过softmax后生成的概率趋近于0，相当于不计算这些位置的注意力权重）</p><h2 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder-Decoder Attention"></a>Encoder-Decoder Attention</h2><p>其实是多头自注意力，K和V来自Encoder的输出，Q：如果是Decoder的第一层，则使用（已解码的）输入序列（最开始则是起始字符）；如果是后面的层，则是前面层的输出</p><h2 id="Output-Generator"><a href="#Output-Generator" class="headerlink" title="Output Generator"></a>Output Generator</h2><p>实质是线性层，将解码的序列映射回原来的空间维度，然后经过softmax（或log-softmax）生成预测概率</p><h2 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/2ee8a87bcd29">Transformer 修炼之道（三）、Decoder</a></p><h1 id="Transformer总结"><a href="#Transformer总结" class="headerlink" title="Transformer总结"></a>Transformer总结</h1><ul><li>Transformer 与 RNN 不同，可以比较好地并行训练</li><li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了</li><li>Transformer 的重点是 Self-Attention 结构，其中用到的 Q, K, V矩阵通过输出进行线性变换得到</li><li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score</li></ul><h1 id="整体可参考"><a href="#整体可参考" class="headerlink" title="整体可参考"></a>整体可参考</h1><p><a href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版）</a></p><h1 id="变种"><a href="#变种" class="headerlink" title="变种"></a>变种</h1><h2 id="Sandwich-Transformer"><a href="#Sandwich-Transformer" class="headerlink" title="Sandwich Transformer"></a>Sandwich Transformer</h2><p>变换了self-attention和FFN的顺序</p><h2 id="Universal-Transformer"><a href="#Universal-Transformer" class="headerlink" title="Universal Transformer"></a>Universal Transformer</h2><p>在深度上加上循环，增加通用性</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GitHub-Hexo博客网站搭建（一）</title>
    <link href="/2022/11/08/GitHub-Hexo%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2022/11/08/GitHub-Hexo%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>最近看到别人在GitHub上的网站，想着自己也弄一个，放一些平时写的笔记或找到的资料什么的。</p><h1 id="网站搭建"><a href="#网站搭建" class="headerlink" title="网站搭建"></a>网站搭建</h1><p>网站的搭建其实很简单，在GitHub上新建一个repository，把名字命名为username.github.io就行。</p><p>网站有了，那怎么写文章？刚开始我还想着要涉及到前端的编程，其实不用。有一个叫框架的东西，可以帮我们实现前端，我们要做的就是写markdown就行了。我没用github默认的框架，手册都是英文而且感觉不清楚，我搞不懂。现在大家好像都用Hexo。</p><p>那么现在就是要把网站换成Hexo框架。需要下两个东西，Git和Node.js。</p><p>Git一般玩GitHub的都有下，主要目的就是为了方便项目管理。在本地更新项目后git上去比在GitHub上操作方便多了。</p><p>Node.js是Hexo的基础（我也不是很懂），就是Hexo是基于Node.js的。</p><p>详细的搭建步骤<a href="https://blog.csdn.net/qq_39400113/article/details/104703467?spm=1001.2014.3001.5501">Github+Hexo+matery博客搭建_果果小师弟的博客-CSDN博客</a></p><h1 id="网站主题"><a href="#网站主题" class="headerlink" title="网站主题"></a>网站主题</h1><p>网站主题就是网页显示的样子，主题有很多，萝卜青菜各有所爱，我先用的fluid。</p><p>安装的详细步骤<a href="https://hexo.fluid-dev.com/docs/start/#%E4%B8%BB%E9%A2%98%E7%AE%80%E4%BB%8B">开始使用 | Hexo Fluid 用户手册</a></p><h1 id="多电脑写博客"><a href="#多电脑写博客" class="headerlink" title="多电脑写博客"></a>多电脑写博客</h1><p>因为工位有电脑，自己也有一台，所以如果两台电脑都能写博客， 就会很方便。</p><p>这个方法其实就是把博客的所有内容也都放到GitHub上，然后要写博客就更新一下本地的repository，写好博客后再更新到GitHub上。</p><p>所以Git和Hexo都是必备的。</p><p>具体步骤可以参考：</p><p><a href="https://blog.csdn.net/qq_30105599/article/details/118302086">多台电脑同步更新Hexo博客___tourist的博客-CSDN博客_hexo 多台电脑</a></p><p><a href="https://www.bilibili.com/read/cv16385424">https://www.bilibili.com/read/cv16385424</a></p><h1 id="多电脑写博客改进"><a href="#多电脑写博客改进" class="headerlink" title="多电脑写博客改进"></a>多电脑写博客改进</h1><p>上面的方法是没毛病，但是有个问题，就是这样做的话源文件都公开了，安全性不太好。虽然可能也没多少人看，也没人愿意抄，真想搞到文章也拦不住，但是还是保护一下源文件，不然心里膈应。</p><p>刚开始试了把repository改成私有的，但是这样就没有公开的网站了；也试了lock branch的分支保护规则，但是只是不允许别人push，clone还是可以的呀。</p><p>所以想出一个解决方法就是，把源文件放在一个新的repository里，让这个repository是私有的，发布的网页还是放在原来的repository里。</p><p>操作很简单：</p><ol><li><p>新建一个私有的repository</p></li><li><p>在私有repository里随便添加个文件（readme什么的都行），然后clone到本地</p></li><li><p>把源文件（除了.git和.deploy_git文件夹）全部copy到私有的repository里</p></li><li><p>把github上发布网站的repository的存放源文件的分支删掉</p></li><li><p>在私有repository里push一下把源文件存到私有库里</p></li></ol><p>这样就完成了，之后所有的操作都在新的这个私有repository里，命令什么的也都一样。</p>]]></content>
    
    
    
    <tags>
      
      <tag>网站搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
